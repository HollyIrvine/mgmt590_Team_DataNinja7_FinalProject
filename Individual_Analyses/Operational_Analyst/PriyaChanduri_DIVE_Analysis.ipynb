{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HollyIrvine/mgmt590_Team_DataNinja7_FinalProject/blob/main/Individual_Analyses/Operational_Analyst/PriyaChanduri_Operational_Analyst_DIVE_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt : I'm an Operational Excellence Analyst and I want to analyze an e-commerce dataset to identify areas for improvement. Provide an outline of the key features of the analysis using the DIVE framework (Discover, Investigate, Validate, Extend)."
      ],
      "metadata": {
        "id": "FK9rDq_gkFlt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Features of the Analysis:\n",
        "\n",
        "Operational Excellence Analyst:\n",
        "•\tDiscover: Process efficiency, resource utilization, operational metrics\n",
        "•\tInvestigate: What operational factors drive success? Where are bottlenecks?\n",
        "•\tValidate: Test operational improvement hypotheses\n",
        "•\tExtend: Operational optimization roadmap"
      ],
      "metadata": {
        "id": "5cn64dgfkbNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discover - Process Efficiency Metrics:\n",
        "\n",
        "Shipping efficiency - average days from order to shipment\n",
        "Regional performance - operational metrics by geographic region\n",
        "Category analysis - volume and efficiency by product categories\n",
        "Monthly trends - seasonal patterns and performance evolution\n"
      ],
      "metadata": {
        "id": "-1kJLZAOlVCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt : Let's start with the \"Discover\" phase. I want to analyze shipping efficiency metrics from the EcomStoreSalesPipeline.EcomStoreDataTable BigQuery table. Calculate the overall average, median, min, and max shipping days, as well as the total orders and the percentage of orders shipped within 2 days (classified as 'Fast')."
      ],
      "metadata": {
        "id": "tWX7-W8clK-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "%%bigquery results\n",
        "WITH shipping_metrics AS (\n",
        "  SELECT\n",
        "    *,\n",
        "    DATE_DIFF(ShipDate, OrderDate, DAY) AS shipping_days,\n",
        "    CASE\n",
        "      WHEN DATE_DIFF(ShipDate, OrderDate, DAY) <= 2 THEN 'Fast'\n",
        "      WHEN DATE_DIFF(ShipDate, OrderDate, DAY) <= 4 THEN 'Standard'\n",
        "      ELSE 'Slow'\n",
        "    END AS shipping_performance_category\n",
        "  FROM `mgmt599-final-project.EcomStoreSalesPipeline.EcomStoreDataTable`\n",
        "  WHERE ShipDate IS NOT NULL\n",
        "    AND OrderDate IS NOT NULL\n",
        "    AND DATE_DIFF(ShipDate, OrderDate, DAY) >= 0\n",
        ")\n",
        "\n",
        "SELECT\n",
        "  'Overall Shipping Performance' AS metric_category,\n",
        "  ROUND(AVG(shipping_days), 1) AS avg_shipping_days,\n",
        "  APPROX_QUANTILES(shipping_days, 100)[OFFSET(50)] AS median_shipping_days,\n",
        "  MIN(shipping_days) AS min_shipping_days,\n",
        "  MAX(shipping_days) AS max_shipping_days,\n",
        "  COUNT(*) AS total_orders,\n",
        "  COUNTIF(shipping_days <= 2) AS fast_orders,\n",
        "  ROUND(COUNTIF(shipping_days <= 2) / COUNT(*) * 100, 1) AS fast_order_pct\n",
        "FROM shipping_metrics;"
      ],
      "metadata": {
        "id": "sF_AGyIjmPhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "prompt: Display the results of the previous BigQuery query."
      ],
      "metadata": {
        "id": "aCQZn7Dvldeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "results"
      ],
      "metadata": {
        "id": "in1WaYz5ypVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt : Now, analyze shipping performance by ship mode. For each ShipMode, calculate the order count, average and median shipping days, average order value, average profit per order, profit margin percentage, number of fast deliveries (within 2 days), and the fast delivery rate percentage. Order the results by average shipping days ascending."
      ],
      "metadata": {
        "id": "UO4i6F1ul7u5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. SHIPPING PERFORMANCE BY MODE"
      ],
      "metadata": {
        "id": "IoeZVOd6mey0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "%%bigquery results\n",
        "\n",
        "WITH shipping_metrics AS (\n",
        "  SELECT\n",
        "    *,\n",
        "    DATE_DIFF(ShipDate, OrderDate, DAY) AS shipping_days\n",
        "  FROM `mgmt599-final-project.EcomStoreSalesPipeline.EcomStoreDataTable`\n",
        "  WHERE ShipDate IS NOT NULL\n",
        "    AND OrderDate IS NOT NULL\n",
        "    AND DATE_DIFF(ShipDate, OrderDate, DAY) >= 0\n",
        ")\n",
        "\n",
        "SELECT\n",
        "  ShipMode,\n",
        "  COUNT(*) AS order_count,\n",
        "  ROUND(AVG(shipping_days), 1) AS avg_shipping_days,\n",
        "  APPROX_QUANTILES(shipping_days, 100)[OFFSET(50)] AS median_shipping_days,\n",
        "  ROUND(AVG(Sales), 2) AS avg_order_value,\n",
        "  ROUND(AVG(Profit), 2) AS avg_profit_per_order,\n",
        "  ROUND(AVG(Profit) / AVG(Sales) * 100, 1) AS profit_margin_pct,\n",
        "  COUNTIF(shipping_days <= 2) AS fast_deliveries,\n",
        "  ROUND(COUNTIF(shipping_days <= 2) / COUNT(*) * 100, 1) AS fast_delivery_rate_pct\n",
        "FROM shipping_metrics\n",
        "GROUP BY ShipMode\n",
        "ORDER BY avg_shipping_days ASC;"
      ],
      "metadata": {
        "id": "ZF1X03xPmg6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "prompt: Display the results of the previous BigQuery query"
      ],
      "metadata": {
        "id": "FztRpWHNmE7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "results"
      ],
      "metadata": {
        "id": "vRL3HrCM-LLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2HLWPunfmMQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt 6: Next, analyze regional operational efficiency. For each Region, calculate the total orders, unique customers, average orders per customer, average shipping days, total sales, total profit, profit margin percentage, average order value, number of loss-making orders (profit < 0), and the percentage of loss orders. Order the results by total sales descending."
      ],
      "metadata": {
        "id": "5HjbBhCGmWh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. REGIONAL OPERATIONAL EFFICIENCY"
      ],
      "metadata": {
        "id": "4wCUT3nEmtIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "%%bigquery results\n",
        "WITH regional_ops AS (\n",
        "  SELECT\n",
        "    *,\n",
        "    DATE_DIFF(ShipDate, OrderDate, DAY) AS shipping_days\n",
        "  FROM `mgmt599-final-project.EcomStoreSalesPipeline.EcomStoreDataTable`\n",
        "  WHERE ShipDate IS NOT NULL\n",
        "    AND OrderDate IS NOT NULL\n",
        "    AND DATE_DIFF(ShipDate, OrderDate, DAY) >= 0\n",
        ")\n",
        "\n",
        "SELECT\n",
        "  Region,\n",
        "  COUNT(*) AS total_orders,\n",
        "  COUNT(DISTINCT CustomerID) AS unique_customers,\n",
        "  ROUND(COUNT(*) / COUNT(DISTINCT CustomerID), 1) AS avg_orders_per_customer,\n",
        "  ROUND(AVG(shipping_days), 1) AS avg_shipping_days,\n",
        "  ROUND(SUM(Sales), 2) AS total_sales,\n",
        "  ROUND(SUM(Profit), 2) AS total_profit,\n",
        "  ROUND(SUM(Profit) / SUM(Sales) * 100, 1) AS profit_margin_pct,\n",
        "  ROUND(AVG(Sales), 2) AS avg_order_value,\n",
        "  COUNTIF(Profit < 0) AS loss_making_orders,\n",
        "  ROUND(COUNTIF(Profit < 0) / COUNT(*) * 100, 1) AS loss_order_pct\n",
        "FROM regional_ops\n",
        "GROUP BY Region\n",
        "ORDER BY total_sales DESC;"
      ],
      "metadata": {
        "id": "eAsP-6UAmspe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt : Display the results of the previous BigQuery query."
      ],
      "metadata": {
        "id": "-L4dOVi5moXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "results"
      ],
      "metadata": {
        "id": "56_vWaei_XhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt 8: Analyze product category operational metrics. For each Category, calculate the order volume, total units sold, average units per order, total revenue, total profit, profit margin percentage, average shipping days, average discount percentage, number of unprofitable orders (profit < 0), and the percentage of unprofitable orders. Order the results by total revenue descending."
      ],
      "metadata": {
        "id": "4EfRvJU1mmx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. PRODUCT CATEGORY OPERATIONAL METRICS"
      ],
      "metadata": {
        "id": "GyMFJtG-m4I7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "%%bigquery results\n",
        "WITH category_ops AS (\n",
        "  SELECT\n",
        "    *,\n",
        "    DATE_DIFF(ShipDate, OrderDate, DAY) AS shipping_days\n",
        "  FROM `mgmt599-final-project.EcomStoreSalesPipeline.EcomStoreDataTable`\n",
        "  WHERE ShipDate IS NOT NULL\n",
        "    AND OrderDate IS NOT NULL\n",
        ")\n",
        "\n",
        "SELECT\n",
        "  Category,\n",
        "  COUNT(*) AS order_volume,\n",
        "  ROUND(SUM(Quantity), 0) AS total_units_sold,\n",
        "  ROUND(AVG(Quantity), 1) AS avg_units_per_order,\n",
        "  ROUND(SUM(Sales), 2) AS total_revenue,\n",
        "  ROUND(SUM(Profit), 2) AS total_profit,\n",
        "  ROUND(SUM(Profit) / SUM(Sales) * 100, 1) AS profit_margin_pct,\n",
        "  ROUND(AVG(shipping_days), 1) AS avg_shipping_days,\n",
        "  ROUND(AVG(Discount) * 100, 1) AS avg_discount_pct,\n",
        "  COUNTIF(Profit < 0) AS unprofitable_orders,\n",
        "  ROUND(COUNTIF(Profit < 0) / COUNT(*) * 100, 1) AS unprofitable_order_pct\n",
        "FROM category_ops\n",
        "GROUP BY Category\n",
        "ORDER BY total_revenue DESC;"
      ],
      "metadata": {
        "id": "d6RBrwcRm32T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "prompt : Display the results of the previous BigQuery query."
      ],
      "metadata": {
        "id": "3OrvBq6hm3C6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cjXa3iNGYdW",
        "outputId": "5831f186-dd2f-4df9-a592-33454215fca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QueryJob<project=, location=None, id=45a6cca7-c42f-45e1-ad55-a0f51e05f7e3>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt 10: Analyze customer segment efficiency. For each Segment, calculate the total orders, unique customers, average orders per customer, total revenue, revenue per customer, average order value, profit margin percentage, average shipping days, and average discount rate percentage. Order the results by revenue per customer descending."
      ],
      "metadata": {
        "id": "bjoslaBTnPd-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- 5. CUSTOMER SEGMENT EFFICIENCY"
      ],
      "metadata": {
        "id": "WXSiCDS_nYrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "%%bigquery results\n",
        "WITH segment_analysis AS (\n",
        "  SELECT\n",
        "    *,\n",
        "    DATE_DIFF(ShipDate, OrderDate, DAY) AS shipping_days\n",
        "  FROM `mgmt599-final-project.EcomStoreSalesPipeline.EcomStoreDataTable`\n",
        "  WHERE ShipDate IS NOT NULL\n",
        "    AND OrderDate IS NOT NULL\n",
        ")\n",
        "\n",
        "SELECT\n",
        "  Segment,\n",
        "  COUNT(*) AS total_orders,\n",
        "  COUNT(DISTINCT CustomerID) AS unique_customers,\n",
        "  ROUND(COUNT(*) / COUNT(DISTINCT CustomerID), 1) AS avg_orders_per_customer,\n",
        "  ROUND(SUM(Sales), 2) AS total_revenue,\n",
        "  ROUND(SUM(Sales) / COUNT(DISTINCT CustomerID), 2) AS revenue_per_customer,\n",
        "  ROUND(AVG(Sales), 2) AS avg_order_value,\n",
        "  ROUND(SUM(Profit) / SUM(Sales) * 100, 1) AS profit_margin_pct,\n",
        "  ROUND(AVG(shipping_days), 1) AS avg_shipping_days,\n",
        "  ROUND(AVG(Discount) * 100, 1) AS avg_discount_rate_pct\n",
        "FROM segment_analysis\n",
        "GROUP BY Segment\n",
        "ORDER BY revenue_per_customer DESC;"
      ],
      "metadata": {
        "id": "Y6B7jZZvncQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt 11: Display the results of the previous BigQuery query."
      ],
      "metadata": {
        "id": "Ia5IgW5Jm9Jd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2k4OaF5EGokP",
        "outputId": "e945535d-f8aa-42f5-c9bf-832c563adc66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QueryJob<project=, location=None, id=17c8ad70-e68a-4a21-8fa8-c95d4ed6609e>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt : Analyze monthly operational trends. For each order_year and order_month, calculate the order count, monthly revenue, monthly profit, profit margin percentage, average shipping days, number of fast deliveries (within 2 days), and the fast delivery rate percentage. Order the results by year and month."
      ],
      "metadata": {
        "id": "3groCylgnn2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "-- 6. MONTHLY OPERATIONAL TRENDS"
      ],
      "metadata": {
        "id": "IhDsjzZ3ngMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "%%bigquery results\n",
        "WITH monthly_trends AS (\n",
        "  SELECT\n",
        "    *,\n",
        "    EXTRACT(YEAR FROM OrderDate) AS order_year,\n",
        "    EXTRACT(MONTH FROM OrderDate) AS order_month,\n",
        "    DATE_DIFF(ShipDate, OrderDate, DAY) AS shipping_days\n",
        "  FROM `mgmt599-final-project.EcomStoreSalesPipeline.EcomStoreDataTable`\n",
        "  WHERE OrderDate IS NOT NULL\n",
        ")\n",
        "\n",
        "SELECT\n",
        "  order_year,\n",
        "  order_month,\n",
        "  FORMAT_DATE('%B', DATE(order_year, order_month, 1)) AS month_name,\n",
        "  COUNT(*) AS order_count,\n",
        "  ROUND(SUM(Sales), 2) AS monthly_revenue,\n",
        "  ROUND(SUM(Profit), 2) AS monthly_profit,\n",
        "  ROUND(SUM(Profit) / SUM(Sales) * 100, 1) AS profit_margin_pct,\n",
        "  ROUND(AVG(shipping_days), 1) AS avg_shipping_days,\n",
        "  COUNTIF(shipping_days <= 2) AS fast_deliveries,\n",
        "  ROUND(COUNTIF(shipping_days <= 2) / COUNT(*) * 100, 1) AS fast_delivery_rate_pct\n",
        "FROM monthly_trends\n",
        "GROUP BY order_year, order_month\n",
        "ORDER BY order_year, order_month;"
      ],
      "metadata": {
        "id": "UGrg0Fj5kot2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "prompt 13: Display the results of the previous BigQuery query."
      ],
      "metadata": {
        "id": "R2rGfjePnD86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "results"
      ],
      "metadata": {
        "id": "5WvbyBZ2G3n9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt 14: Identify operational bottlenecks by looking at slow-shipping product sub-categories. For each Category and Sub-Category, calculate the order count, average shipping days, total sales, and profit margin percentage. Filter for sub-categories with at least 50 orders. Categorize the bottleneck priority based on average shipping days (High > 5 days, Medium > 3 days, Low <= 3 days). Order by average shipping days descending, then total sales descending."
      ],
      "metadata": {
        "id": "FSftJ58UnzdD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- 7. BOTTLENECK IDENTIFICATION"
      ],
      "metadata": {
        "id": "6AVCVnqSqkj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "%%bigquery results\n",
        "-- Identify slow-shipping product categories\n",
        "WITH shipping_bottlenecks AS (\n",
        "  SELECT\n",
        "    Category,\n",
        "    `Sub-Category`,\n",
        "    COUNT(*) AS order_count,\n",
        "    ROUND(AVG(DATE_DIFF(ShipDate, OrderDate, DAY)), 1) AS avg_shipping_days,\n",
        "    ROUND(SUM(Sales), 2) AS total_sales,\n",
        "    ROUND(SUM(Profit) / SUM(Sales) * 100, 1) AS profit_margin_pct\n",
        "  FROM `mgmt599-final-project.EcomStoreSalesPipeline.EcomStoreDataTable`\n",
        "  WHERE ShipDate IS NOT NULL\n",
        "    AND OrderDate IS NOT NULL\n",
        "  GROUP BY Category, `Sub-Category`\n",
        "  HAVING COUNT(*) >= 50  -- Filter for significant volume\n",
        ")\n",
        "SELECT\n",
        "  Category,\n",
        "  `Sub-Category`,\n",
        "  order_count,\n",
        "  avg_shipping_days,\n",
        "  total_sales,\n",
        "  profit_margin_pct,\n",
        "  CASE\n",
        "    WHEN avg_shipping_days > 5 THEN 'High Priority - Slow Shipping'\n",
        "    WHEN avg_shipping_days > 3 THEN 'Medium Priority - Standard Shipping'\n",
        "    ELSE 'Low Priority - Fast Shipping'\n",
        "  END AS bottleneck_priority\n",
        "FROM shipping_bottlenecks\n",
        "ORDER BY avg_shipping_days DESC, total_sales DESC;"
      ],
      "metadata": {
        "id": "ni5hpqLkqkOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "results"
      ],
      "metadata": {
        "id": "i7mo-shEIatP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt 16: Analyze profit efficiency by identifying high-volume, low-profit scenarios at the sub-category level. For each Category and Sub-Category, calculate the order count, total sales, total profit, profit margin percentage, average discount percentage, number of loss orders (profit < 0), and the percentage of loss orders. Filter for sub-categories with at least 20 orders. Categorize the optimization priority based on profit margin percentage (Critical < 0, High < 10, Medium < 20, Low >= 20). Include the average order value. Order by profit margin percentage ascending, then total sales descending."
      ],
      "metadata": {
        "id": "IKwUFgW5oAVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- 8. PROFIT EFFICIENCY ANALYSIS"
      ],
      "metadata": {
        "id": "S_DOPaILplR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "%%bigquery results\n",
        "-- Identify high-volume, low-profit scenarios\n",
        "WITH profit_efficiency AS (\n",
        "  SELECT\n",
        "    Category,\n",
        "    `Sub-Category`,\n",
        "    COUNT(*) AS order_count,\n",
        "    ROUND(SUM(Sales), 2) AS total_sales,\n",
        "    ROUND(SUM(Profit), 2) AS total_profit,\n",
        "    ROUND(SUM(Profit) / SUM(Sales) * 100, 1) AS profit_margin_pct,\n",
        "    ROUND(AVG(Discount) * 100, 1) AS avg_discount_pct,\n",
        "    COUNTIF(Profit < 0) AS loss_orders,\n",
        "    ROUND(COUNTIF(Profit < 0) / COUNT(*) * 100, 1) AS loss_order_pct\n",
        "  FROM `mgmt599-final-project.EcomStoreSalesPipeline.EcomStoreDataTable`\n",
        "  GROUP BY Category, `Sub-Category`\n",
        "  HAVING COUNT(*) >= 20\n",
        ")\n",
        "\n",
        "SELECT\n",
        "  *,\n",
        "  CASE\n",
        "    WHEN profit_margin_pct < 0 THEN 'Critical - Losing Money'\n",
        "    WHEN profit_margin_pct < 10 THEN 'High Priority - Low Margin'\n",
        "    WHEN profit_margin_pct < 20 THEN 'Medium Priority - Moderate Margin'\n",
        "    ELSE 'Low Priority - Good Margin'\n",
        "  END AS optimization_priority,\n",
        "  ROUND(total_sales / order_count, 2) AS avg_order_value\n",
        "FROM profit_efficiency\n",
        "ORDER BY profit_margin_pct ASC, total_sales DESC;"
      ],
      "metadata": {
        "id": "01KP7UgQpk7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "results"
      ],
      "metadata": {
        "id": "gqOSXRmvIsu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Prompt: Create a comprehensive operational dashboard summary query. This single query should provide an overview of key business metrics: total orders, unique customers, unique order transactions, total revenue, total profit, overall profit margin percentage, average order value, average shipping days, fast deliveries, fast delivery rate percentage, unprofitable orders, and unprofitable order percentage"
      ],
      "metadata": {
        "id": "lPy42flsoLzR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-- 9. COMPREHENSIVE OPERATIONAL DASHBOARD QUERY"
      ],
      "metadata": {
        "id": "umfztSC5pUPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "%%bigquery results\n",
        "-- Single query for executive dashboard\n",
        "WITH operational_summary AS (\n",
        "  SELECT\n",
        "    *,\n",
        "    DATE_DIFF(ShipDate, OrderDate, DAY) AS shipping_days,\n",
        "    EXTRACT(YEAR FROM OrderDate) AS order_year,\n",
        "    EXTRACT(MONTH FROM OrderDate) AS order_month\n",
        "  FROM `mgmt599-final-project.EcomStoreSalesPipeline.EcomStoreDataTable`\n",
        "  WHERE OrderDate IS NOT NULL AND ShipDate IS NOT NULL\n",
        ")\n",
        "\n",
        "SELECT\n",
        "  'Business Overview' AS section,\n",
        "  COUNT(*) AS total_orders,\n",
        "  COUNT(DISTINCT CustomerID) AS unique_customers,\n",
        "  COUNT(DISTINCT OrderID) AS unique_order_transactions,\n",
        "  ROUND(SUM(Sales), 2) AS total_revenue,\n",
        "  ROUND(SUM(Profit), 2) AS total_profit,\n",
        "  ROUND(SUM(Profit) / SUM(Sales) * 100, 1) AS overall_profit_margin_pct,\n",
        "  ROUND(AVG(Sales), 2) AS avg_order_value,\n",
        "  ROUND(AVG(shipping_days), 1) AS avg_shipping_days,\n",
        "  COUNTIF(shipping_days <= 2) AS fast_deliveries,\n",
        "  ROUND(COUNTIF(shipping_days <= 2) / COUNT(*) * 100, 1) AS fast_delivery_rate_pct,\n",
        "  COUNTIF(Profit < 0) AS unprofitable_orders,\n",
        "  ROUND(COUNTIF(Profit < 0) / COUNT(*) * 100, 1) AS unprofitable_order_pct\n",
        "FROM operational_summary;"
      ],
      "metadata": {
        "id": "_-p4p73ZkZso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "results"
      ],
      "metadata": {
        "id": "ZtcWhGC0pWQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt : Create a BigQuery view named Operational_Analyst_demand_forecasting_model for demand forecasting with time series features. Include features like daily orders, daily units, daily sales, average processing days, time features (year, month, day of week, quarter, day of year, holiday season, weekend, week of month), moving averages (7-day and 30-day orders), lag features (1, 7, and 30 days for orders), volatility (30-day orders), trend direction, seasonal index, growth rate, a simple trend-based forecast (7 days ahead), and confidence intervals based on volatility. Also, include a capacity recommendation based on the forecast and a forecast accuracy error rate. Filter for orders with valid ship dates and order dates, and where 7-day moving average is not null. Order by region, category, and order date descending."
      ],
      "metadata": {
        "id": "v0nSXB5jocFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "-- MODEL 1: DEMAND FORECASTING WITH TIME SERIES"
      ],
      "metadata": {
        "id": "pV5KTkyD1lbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "%%bigquery results\n",
        "\n",
        "-- Create comprehensive time series for demand prediction\n",
        "CREATE OR REPLACE table  `mgmt599-final-project.EcomStoreSalesPipeline.Operational_Analyst_demand_forecasting_model` AS\n",
        "WITH time_series_base AS (\n",
        "  SELECT\n",
        "    OrderDate AS order_date,\n",
        "    Region,\n",
        "    Category,\n",
        "    ShipMode,\n",
        "    COUNT(*) AS daily_orders,\n",
        "    SUM(Quantity) AS daily_units,\n",
        "    SUM(Sales) AS daily_sales,\n",
        "    AVG(DATE_DIFF(ShipDate,\n",
        "                  OrderDate, DAY)) AS avg_processing_days\n",
        "  FROM `mgmt599-final-project.EcomStoreSalesPipeline.EcomStoreDataTable`\n",
        "  WHERE OrderDate IS NOT NULL AND ShipDate IS NOT NULL\n",
        "  GROUP BY order_date, Region, Category, ShipMode\n",
        "),\n",
        "\n",
        "time_features AS (\n",
        "  SELECT\n",
        "    *,\n",
        "    EXTRACT(YEAR FROM order_date) AS order_year,\n",
        "    EXTRACT(MONTH FROM order_date) AS order_month,\n",
        "    EXTRACT(DAYOFWEEK FROM order_date) AS day_of_week,\n",
        "    EXTRACT(QUARTER FROM order_date) AS order_quarter,\n",
        "    EXTRACT(DAYOFYEAR FROM order_date) AS day_of_year,\n",
        "\n",
        "    -- Seasonality indicators\n",
        "    CASE\n",
        "      WHEN EXTRACT(MONTH FROM order_date) IN (11, 12, 1) THEN 1\n",
        "      ELSE 0\n",
        "    END AS is_holiday_season,\n",
        "\n",
        "    CASE\n",
        "      WHEN EXTRACT(DAYOFWEEK FROM order_date) IN (1, 7) THEN 1\n",
        "      ELSE 0\n",
        "    END AS is_weekend,\n",
        "\n",
        "    -- Business cycle indicators\n",
        "    CASE\n",
        "      WHEN EXTRACT(DAY FROM order_date) BETWEEN 1 AND 7 THEN 'Week1'\n",
        "      WHEN EXTRACT(DAY FROM order_date) BETWEEN 8 AND 14 THEN 'Week2'\n",
        "      WHEN EXTRACT(DAY FROM order_date) BETWEEN 15 AND 21 THEN 'Week3'\n",
        "      ELSE 'Week4'\n",
        "    END AS week_of_month\n",
        "  FROM time_features\n",
        "),\n",
        "\n",
        "moving_averages AS (\n",
        "  SELECT\n",
        "    *,\n",
        "    -- Moving averages for trend analysis\n",
        "    AVG(daily_orders) OVER (\n",
        "      PARTITION BY Region, Category\n",
        "      ORDER BY order_date\n",
        "      ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n",
        "    ) AS orders_7day_ma,\n",
        "\n",
        "    AVG(daily_orders) OVER (\n",
        "      PARTITION BY Region, Category\n",
        "      ORDER BY order_date\n",
        "      ROWS BETWEEN 29 PRECEDING AND CURRENT ROW\n",
        "    ) AS orders_30day_ma,\n",
        "\n",
        "    -- Lag features for autoregression\n",
        "    LAG(daily_orders, 1) OVER (\n",
        "      PARTITION BY Region, Category\n",
        "      ORDER BY order_date\n",
        "    ) AS orders_lag_1,\n",
        "\n",
        "    LAG(daily_orders, 7) OVER (\n",
        "      PARTITION BY Region, Category\n",
        "      ORDER BY order_date\n",
        "    ) AS orders_lag_7,\n",
        "\n",
        "    LAG(daily_orders, 30) OVER (\n",
        "      PARTITION BY Region, Category\n",
        "      ORDER BY order_date\n",
        "    ) AS orders_lag_30,\n",
        "\n",
        "    -- Volatility measures\n",
        "    STDDEV(daily_orders) OVER (\n",
        "      PARTITION BY Region, Category\n",
        "      ORDER BY order_date\n",
        "      ROWS BETWEEN 29 PRECEDING AND CURRENT ROW\n",
        "    ) AS orders_30day_volatility\n",
        "  FROM time_features\n",
        "),\n",
        "\n",
        "predictive_features AS (\n",
        "  SELECT\n",
        "    *,\n",
        "    -- Trend indicators\n",
        "    CASE\n",
        "      WHEN orders_7day_ma > orders_30day_ma * 1.1 THEN 'Upward'\n",
        "      WHEN orders_7day_ma < orders_30day_ma * 0.9 THEN 'Downward'\n",
        "      ELSE 'Stable'\n",
        "    END AS trend_direction,\n",
        "\n",
        "    -- Seasonal adjustment\n",
        "    daily_orders / NULLIF(orders_30day_ma, 0) AS seasonal_index,\n",
        "\n",
        "    -- Growth rate\n",
        "    CASE\n",
        "      WHEN orders_lag_30 > 0 THEN\n",
        "        (daily_orders - orders_lag_30) / orders_lag_30 * 100\n",
        "      ELSE 0\n",
        "    END AS growth_rate_30day,\n",
        "\n",
        "    -- Forecast using linear regression components\n",
        "    -- Simple trend-based forecast (next 7 days)\n",
        "    orders_7day_ma +\n",
        "    (orders_7day_ma - LAG(orders_7day_ma, 7) OVER (\n",
        "      PARTITION BY Region, Category ORDER BY order_date\n",
        "    )) AS trend_forecast_7day\n",
        "  FROM moving_averages\n",
        ")\n",
        "\n",
        "SELECT\n",
        "  order_date,\n",
        "  Region,\n",
        "  Category,\n",
        "  ShipMode,\n",
        "  daily_orders AS actual_orders,\n",
        "  ROUND(orders_7day_ma, 1) AS ma_7day,\n",
        "  ROUND(orders_30day_ma, 1) AS ma_30day,\n",
        "  ROUND(orders_30day_volatility, 2) AS volatility_30day,\n",
        "  trend_direction,\n",
        "  ROUND(seasonal_index, 2) AS seasonal_factor,\n",
        "  ROUND(growth_rate_30day, 1) AS growth_rate_pct,\n",
        "  ROUND(trend_forecast_7day, 1) AS forecast_7day_ahead,\n",
        "\n",
        "  -- Confidence intervals (using volatility)\n",
        "  ROUND(trend_forecast_7day - (orders_30day_volatility * 1.96), 1) AS forecast_lower_ci,\n",
        "  ROUND(trend_forecast_7day + (orders_30day_volatility * 1.96), 1) AS forecast_upper_ci,\n",
        "\n",
        "  -- Resource planning indicators\n",
        "  CASE\n",
        "    WHEN trend_forecast_7day > orders_30day_ma * 1.3 THEN 'Scale Up - 30%+ Increase Expected'\n",
        "    WHEN trend_forecast_7day > orders_30day_ma * 1.15 THEN 'Scale Up - 15%+ Increase Expected'\n",
        "    WHEN trend_forecast_7day < orders_30day_ma * 0.85 THEN 'Scale Down - 15%+ Decrease Expected'\n",
        "    WHEN trend_forecast_7day < orders_30day_ma * 0.7 THEN 'Scale Down - 30%+ Decrease Expected'\n",
        "    ELSE 'Maintain Current Capacity'\n",
        "  END AS capacity_recommendation,\n",
        "\n",
        "  -- Forecast accuracy score (how well recent forecasts performed)\n",
        "  ABS(daily_orders - LAG(trend_forecast_7day, 7) OVER (\n",
        "    PARTITION BY Region, Category ORDER BY order_date\n",
        "  )) / NULLIF(daily_orders, 0) AS forecast_error_rate\n",
        "\n",
        "FROM predictive_features\n",
        "WHERE orders_7day_ma IS NOT NULL\n",
        "ORDER BY Region, Category, order_date DESC;"
      ],
      "metadata": {
        "id": "TXpFOoTO1aRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c26x-an924R6",
        "outputId": "4c1f9cca-6673-4384-ed54-be39beecf7d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QueryJob<project=, location=None, id=4d3e360d-cd86-40fe-a103-aaf5dd70a775>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt: Create a BigQuery view named Operationa_analyst_processing_time_prediction_model for predicting processing times. Include the actual processing days, order complexity features (size, value, discount category), time features (order month, day of week, holiday season), and baseline processing time metrics (average, median, percentiles 75 and 90, std deviation, sample size) grouped by region, category, sub-category, ship mode, order size, and order value category (filter groups with sample size >= 10). Calculate an adjusted prediction based on holiday season, weekend, and high discount. Include confidence intervals, prediction error and error rate (for historical data), processing risk category, and resource allocation recommendation. Filter for valid ship dates and order dates, and processing days between 0 and 30. Order by adjusted prediction descending, then order date descending."
      ],
      "metadata": {
        "id": "MbKnSpFoowQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown\n",
        "%%bigquery\n",
        "-- ===============================================\n",
        "-- MODEL 2: PROCESSING TIME PREDICTION MODEL\n",
        "-- ===============================================\n",
        "\n",
        "-- Predict processing times for capacity planning\n",
        "CREATE OR REPLACE table `mgmt599-final-project.EcomStoreSalesPipeline.Operationa_analyst_processing_time_prediction_model` AS\n",
        "WITH processing_features AS (\n",
        "  SELECT\n",
        "    *,\n",
        "    DATE_DIFF(ShipDate, OrderDate, DAY) AS actual_processing_days,\n",
        "\n",
        "    -- Order complexity features\n",
        "    CASE\n",
        "      WHEN Quantity = 1 THEN 'Single'\n",
        "      WHEN Quantity <= 3 THEN 'Small'\n",
        "      WHEN Quantity <= 5 THEN 'Medium'\n",
        "      ELSE 'Large'\n",
        "    END AS order_size_category,\n",
        "\n",
        "    CASE\n",
        "      WHEN Sales <= 100 THEN 'Low'\n",
        "      WHEN Sales <= 500 THEN 'Medium'\n",
        "      WHEN Sales <= 1000 THEN 'High'\n",
        "      ELSE 'Premium'\n",
        "    END AS order_value_category,\n",
        "\n",
        "    CASE\n",
        "      WHEN Discount = 0 THEN 'No_Discount'\n",
        "      WHEN Discount <= 0.1 THEN 'Low_Discount'\n",
        "      WHEN Discount <= 0.3 THEN 'Medium_Discount'\n",
        "      ELSE 'High_Discount'\n",
        "    END AS discount_category,\n",
        "\n",
        "    -- Time features\n",
        "    EXTRACT(MONTH FROM OrderDate) AS order_month,\n",
        "    EXTRACT(DAYOFWEEK FROM OrderDate) AS order_day_of_week,\n",
        "\n",
        "    CASE\n",
        "      WHEN EXTRACT(MONTH FROM OrderDate) IN (11, 12, 1) THEN 1\n",
        "      ELSE 0\n",
        "    END AS is_holiday_season\n",
        "\n",
        "  FROM `mgmt599-final-project.EcomStoreSalesPipeline.EcomStoreDataTable`\n",
        "  WHERE ShipDate IS NOT NULL\n",
        "    AND OrderDate IS NOT NULL\n",
        "    AND DATE_DIFF(ShipDate, OrderDate, DAY) >= 0\n",
        "    AND DATE_DIFF(ShipDate, OrderDate, DAY) <= 30\n",
        "),\n",
        "\n",
        "category_baselines AS (\n",
        "  SELECT\n",
        "    Region,\n",
        "    Category,\n",
        "    `Sub-Category`,\n",
        "    ShipMode,\n",
        "    order_size_category,\n",
        "    order_value_category,\n",
        "    AVG(actual_processing_days) AS avg_processing_days,\n",
        "    STDDEV(actual_processing_days) AS std_processing_days,\n",
        "    COUNT(*) AS sample_size,\n",
        "    APPROX_QUANTILES(actual_processing_days, 100)[OFFSET(50)] AS median_processing_days,\n",
        "    APPROX_QUANTILES(actual_processing_days, 100)[OFFSET(75)] AS p75_processing_days,\n",
        "    APPROX_QUANTILES(actual_processing_days, 100)[OFFSET(90)] AS p90_processing_days\n",
        "  FROM processing_features\n",
        "  GROUP BY Region, Category, `Sub-Category`, ShipMode, order_size_category, order_value_category\n",
        "  HAVING COUNT(*) >= 10  -- Ensure statistical significance\n",
        "),\n",
        "\n",
        "predictions AS (\n",
        "  SELECT\n",
        "    pf.*,\n",
        "    cb.avg_processing_days AS baseline_prediction,\n",
        "    cb.std_processing_days AS prediction_uncertainty,\n",
        "    cb.median_processing_days,\n",
        "    cb.p75_processing_days,\n",
        "    cb.p90_processing_days,\n",
        "    cb.sample_size,\n",
        "\n",
        "    -- Adjust baseline prediction based on additional factors\n",
        "    cb.avg_processing_days *\n",
        "    CASE\n",
        "      -- Holiday season adjustment\n",
        "      WHEN pf.is_holiday_season = 1 THEN 1.2\n",
        "      -- Weekend adjustment\n",
        "      WHEN pf.order_day_of_week IN (1, 7) THEN 1.1\n",
        "      -- High discount complexity adjustment\n",
        "      WHEN pf.discount_category = 'High_Discount' THEN 1.15\n",
        "      ELSE 1.0\n",
        "    END AS adjusted_prediction,\n",
        "\n",
        "    -- Confidence intervals\n",
        "    cb.avg_processing_days - (cb.std_processing_days * 1.96) AS prediction_lower_ci,\n",
        "    cb.avg_processing_days + (cb.std_processing_days * 1.96) AS prediction_upper_ci\n",
        "\n",
        "  FROM processing_features pf\n",
        "  LEFT JOIN category_baselines cb\n",
        "    ON pf.Region = cb.Region\n",
        "    AND pf.Category = cb.Category\n",
        "    AND pf.`Sub-Category` = cb.`Sub-Category`\n",
        "    AND pf.ShipMode = cb.ShipMode\n",
        "    AND pf.order_size_category = cb.order_size_category\n",
        "    AND pf.order_value_category = cb.order_value_category\n",
        ")\n",
        "\n",
        "SELECT\n",
        "  RowID,\n",
        "  OrderID,\n",
        "  Region,\n",
        "  Category,\n",
        "  `Sub-Category`,\n",
        "  ShipMode,\n",
        "  order_size_category,\n",
        "  order_value_category,\n",
        "  discount_category,\n",
        "  actual_processing_days,\n",
        "  ROUND(baseline_prediction, 1) AS predicted_processing_days,\n",
        "  ROUND(adjusted_prediction, 1) AS adjusted_predicted_days,\n",
        "  ROUND(prediction_uncertainty, 2) AS prediction_std_dev,\n",
        "  ROUND(prediction_lower_ci, 1) AS prediction_lower_bound,\n",
        "  ROUND(prediction_upper_ci, 1) AS prediction_upper_bound,\n",
        "\n",
        "  -- Prediction accuracy (for historical data)\n",
        "  ABS(actual_processing_days - baseline_prediction) AS prediction_error,\n",
        "  ABS(actual_processing_days - baseline_prediction) / NULLIF(actual_processing_days, 0) AS prediction_error_rate,\n",
        "\n",
        "  -- Risk categories\n",
        "  CASE\n",
        "    WHEN adjusted_prediction > 5 THEN 'High Risk - Slow Processing Expected'\n",
        "    WHEN adjusted_prediction > 3 THEN 'Medium Risk - Standard Processing'\n",
        "    ELSE 'Low Risk - Fast Processing Expected'\n",
        "  END AS processing_risk_category,\n",
        "\n",
        "  -- Resource allocation recommendations\n",
        "  CASE\n",
        "    WHEN adjusted_prediction > 4 AND order_value_category IN ('High', 'Premium') THEN 'Priority Queue - Expedite'\n",
        "    WHEN adjusted_prediction > 5 THEN 'Additional Resources Needed'\n",
        "    WHEN adjusted_prediction > 3 THEN 'Standard Resource Allocation'\n",
        "    ELSE 'Standard Processing Track'\n",
        "  END AS resource_recommendation,\n",
        "\n",
        "  sample_size\n",
        "\n",
        "FROM predictions\n",
        "WHERE baseline_prediction IS NOT NULL\n",
        "ORDER BY adjusted_prediction DESC, OrderDate DESC;"
      ],
      "metadata": {
        "id": "asuXlF_u01Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown"
      ],
      "metadata": {
        "id": "SQdiE3wSyZWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbb9827a"
      },
      "source": [
        "## DIVE Analysis of the E-commerce Operations Notebook\n",
        "\n",
        "This analysis provides a comprehensive overview of the notebook's findings based on the **Discover, Investigate, Validate, Extend (DIVE)** framework.\n",
        "\n",
        "### 1. Discover: Key Operational Insights\n",
        "\n",
        "The initial queries have uncovered several critical patterns and performance metrics:\n",
        "\n",
        "*   **Overall Shipping Performance**: The average shipping time is **4.0 days**, with only **22.2%** of orders qualifying as 'Fast' (shipped within 2 days). This suggests room for improvement in overall logistics speed.\n",
        "*   **Shipping Mode Disparities**: 'Standard Class' is the most used mode but also the slowest, with an average of **5.0 days** and **0%** fast deliveries. In contrast, 'Same Day' and 'First Class' are significantly faster.\n",
        "*   **Regional Performance Gaps**: The **West** region leads in sales and has a strong profit margin (15%). The **Central** region is a major concern, with the lowest profit margin (**8%**) and the highest rate of loss-making orders (**31.9%**).\n",
        "*   **Product Category Profitability Issues**: The **Furniture** category, despite generating significant revenue, has a critically low profit margin of just **2.5%**. This is largely driven by a high percentage of unprofitable orders (33.6%) and high average discounts (17.4%). Technology and Office Supplies are much more profitable (17.4% and 17.1% margins, respectively).\n",
        "*   **Bottlenecks Identified**: The analysis pinpoints specific sub-categories like 'Tables' and 'Bookcases' as consistent loss-makers, while others like 'Supplies' also have negative profit margins.\n",
        "\n",
        "### 2. Investigate: Probing for Root Causes\n",
        "\n",
        "The discoveries raise important questions that require deeper investigation:\n",
        "\n",
        "*   **Central Region's Low Profitability**: *Why* is the Central region underperforming so drastically? We should investigate if this is due to higher shipping costs, a different product mix, more aggressive discounting strategies, or higher rates of product returns in this region.\n",
        "*   **Furniture's Profitability Drain**: *What* is driving the high rate of unprofitable orders in the Furniture category? Is it due to the cost of goods, high damage rates during shipping, or the heavy discounts being offered? The query on profit efficiency strongly suggests discounts are a major factor.\n",
        "*   **'Standard Class' Inefficiency**: *Why* is the most common shipping method the least efficient? Is this a trade-off for lower costs? An analysis comparing the shipping cost vs. profit margin for each mode would be insightful.\n",
        "\n",
        "### 3. Validate: Confirming the Hypotheses\n",
        "\n",
        "Before taking action, we should validate these initial findings:\n",
        "\n",
        "*   **Drill Down into Problem Areas**: We can validate the regional and category issues by performing more granular queries. For the Central region, we can analyze performance by **State** or **City**. For the Furniture category, we can analyze the profitability of every single **product** within the 'Tables' and 'Bookcases' sub-categories.\n",
        "*   **Statistical Significance**: The queries correctly filter for a minimum number of orders (e.g., `HAVING COUNT(*) >= 20`) to ensure the insights are based on a reliable sample size. This is a good practice that validates the significance of the findings.\n",
        "*   **Model Backtesting**: The demand forecasting model created in the final step can be validated by **backtesting** it against historical data to check its prediction accuracy before using it for future planning.\n",
        "\n",
        "### 4. Extend: Future Analysis and Strategic Actions\n",
        "\n",
        "The notebook provides a strong foundation for several high-impact extensions:\n",
        "\n",
        "*   **Build and Train ML Models**: The two views created (`Operational_Analyst_demand_forecasting_model` and `processing_time_prediction_model`) are feature-rich tables ready for machine learning. The next logical step is to use **BigQuery ML** to train a `ARIMA_PLUS` model for demand forecasting and a `BOOSTED_TREE_REGRESSOR` model for predicting processing times. This would shift the analysis from descriptive to predictive.\n",
        "*   **Develop an Interactive Dashboard**: Connect this BigQuery project to **Looker Studio** or another BI tool. This would create a dynamic dashboard where stakeholders can filter by region, category, or date to explore the data and track KPIs in real-time.\n",
        "*   **Actionable Business Recommendations**: Based on the validated insights, we can propose concrete actions:\n",
        "    *   **Pricing Review**: Conduct a review of the pricing and discount strategy for the **Furniture** category, especially for loss-making sub-categories.\n",
        "    *   **Regional Operational Audit**: Initiate an audit of the **Central** region's operations to identify the root causes of its low profitability.\n",
        "    *   **Logistics Optimization**: Re-evaluate the carrier contracts and logistics for the **'Standard Class'** shipping mode to improve its speed and efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt: Summarize the findings of the notebook using the DIVE framework (Discover, Investigate, Validate, Extend) and save the analysis as a markdown file."
      ],
      "metadata": {
        "id": "UynV4UeWpwyr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "3a27dd15",
        "outputId": "25f2d23c-fc2f-4122-8192-dafcca1ce524"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "# The content of the DIVE analysis markdown cell\n",
        "dive_content = \"\"\"## DIVE Analysis of the E-commerce Operations Notebook\n",
        "\n",
        "This analysis provides a comprehensive overview of the notebook's findings based on the **Discover, Investigate, Validate, Extend (DIVE)** framework.\n",
        "\n",
        "### 1. Discover: Key Operational Insights\n",
        "\n",
        "The initial queries have uncovered several critical patterns and performance metrics:\n",
        "\n",
        "*   **Overall Shipping Performance**: The average shipping time is **4.0 days**, with only **22.2%** of orders qualifying as 'Fast' (shipped within 2 days). This suggests room for improvement in overall logistics speed.\n",
        "*   **Shipping Mode Disparities**: 'Standard Class' is the most used mode but also the slowest, with an average of **5.0 days** and **0%** fast deliveries. In contrast, 'Same Day' and 'First Class' are significantly faster.\n",
        "*   **Regional Performance Gaps**: The **West** region leads in sales and has a strong profit margin (15%). The **Central** region is a major concern, with the lowest profit margin (**8%**) and the highest rate of loss-making orders (**31.9%**).\n",
        "*   **Product Category Profitability Issues**: The **Furniture** category, despite generating significant revenue, has a critically low profit margin of just **2.5%**. This is largely driven by a high percentage of unprofitable orders (33.6%) and high average discounts (17.4%). Technology and Office Supplies are much more profitable (17.4% and 17.1% margins, respectively).\n",
        "*   **Bottlenecks Identified**: The analysis pinpoints specific sub-categories like 'Tables' and 'Bookcases' as consistent loss-makers, while others like 'Supplies' also have negative profit margins.\n",
        "\n",
        "### 2. Investigate: Probing for Root Causes\n",
        "\n",
        "The discoveries raise important questions that require deeper investigation:\n",
        "\n",
        "*   **Central Region's Low Profitability**: *Why* is the Central region underperforming so drastically? We should investigate if this is due to higher shipping costs, a different product mix, more aggressive discounting strategies, or higher rates of product returns in this region.\n",
        "*   **Furniture's Profitability Drain**: *What* is driving the high rate of unprofitable orders in the Furniture category? Is it due to the cost of goods, high damage rates during shipping, or the heavy discounts being offered? The query on profit efficiency strongly suggests discounts are a major factor.\n",
        "*   **'Standard Class' Inefficiency**: *Why* is the most common shipping method the least efficient? Is this a trade-off for lower costs? An analysis comparing the shipping cost vs. profit margin for each mode would be insightful.\n",
        "\n",
        "### 3. Validate: Confirming the Hypotheses\n",
        "\n",
        "Before taking action, we should validate these initial findings:\n",
        "\n",
        "*   **Drill Down into Problem Areas**: We can validate the regional and category issues by performing more granular queries. For the Central region, we can analyze performance by **State** or **City**. For the Furniture category, we can analyze the profitability of every single **product** within the 'Tables' and 'Bookcases' sub-categories.\n",
        "*   **Statistical Significance**: The queries correctly filter for a minimum number of orders (e.g., `HAVING COUNT(*) >= 20`) to ensure the insights are based on a reliable sample size. This is a good practice that validates the significance of the findings.\n",
        "*   **Model Backtesting**: The demand forecasting model created in the final step can be validated by **backtesting** it against historical data to check its prediction accuracy before using it for future planning.\n",
        "\n",
        "### 4. Extend: Future Analysis and Strategic Actions\n",
        "\n",
        "The notebook provides a strong foundation for several high-impact extensions:\n",
        "\n",
        "*   **Build and Train ML Models**: The two views created (`Operational_Analyst_demand_forecasting_model` and `processing_time_prediction_model`) are feature-rich tables ready for machine learning. The next logical step is to use **BigQuery ML** to train a `ARIMA_PLUS` model for demand forecasting and a `BOOSTED_TREE_REGRESSOR` model for predicting processing times. This would shift the analysis from descriptive to predictive.\n",
        "*   **Develop an Interactive Dashboard**: Connect this BigQuery project to **Looker Studio** or another BI tool. This would create a dynamic dashboard where stakeholders can filter by region, category, or date to explore the data and track KPIs in real-time.\n",
        "*   **Actionable Business Recommendations**: Based on the validated insights, we can propose concrete actions:\n",
        "    *   **Pricing Review**: Conduct a review of the pricing and discount strategy for the **Furniture** category, especially for loss-making sub-categories.\n",
        "    *   **Regional Operational Audit**: Initiate an audit of the **Central** region's operations to identify the root causes of its low profitability.\n",
        "    *   **Logistics Optimization**: Re-evaluate the carrier contracts and logistics for the **'Standard Class'** shipping mode to improve its speed and efficiency.\n",
        "\"\"\"\n",
        "\n",
        "file_name = \"DIVE_Analysis.md\"\n",
        "\n",
        "with open(file_name, \"w\") as f:\n",
        "    f.write(dive_content)\n",
        "\n",
        "files.download(file_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_edc918d6-7c8d-47a0-9015-2e3ec154e8fa\", \"DIVE_Analysis.md\", 4882)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
